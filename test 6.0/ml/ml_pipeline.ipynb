{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bcb51c",
   "metadata": {},
   "source": [
    "### Imports e utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c710f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b411f",
   "metadata": {},
   "source": [
    "### Utilitários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7321b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÉLULA — Parser robusto p/ JSONL com dicts em string (aspas simples / None)\n",
    "import json, re\n",
    "from ast import literal_eval\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def _try_literal_eval(s: str) -> Any:\n",
    "    \"\"\"Tenta converter string 'estilo Python' (ex.: \"{'k': 'v'}\", 'None') em objeto Python.\"\"\"\n",
    "    s = s.strip()\n",
    "    if s in (\"None\", \"none\", \"NULL\", \"Null\", \"null\"):\n",
    "        return None\n",
    "    # se parece com dict/list/tuple/num/str python → tenta literal_eval\n",
    "    if (s.startswith(\"{\") and s.endswith(\"}\")) or \\\n",
    "       (s.startswith(\"[\") and s.endswith(\"]\")) or \\\n",
    "       (s.startswith(\"(\") and s.endswith(\")\")) or \\\n",
    "       (s.startswith(\"'\") and s.endswith(\"'\")) or \\\n",
    "       (s.startswith('\"') and s.endswith('\"')):\n",
    "        try:\n",
    "            return literal_eval(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # último recurso: se parece com JSON de verdade (aspas duplas), tenta json.loads\n",
    "    if (s.startswith(\"{\") and s.endswith(\"}\")) or (s.startswith(\"[\") and s.endswith(\"]\")):\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return s  # mantém como string\n",
    "\n",
    "def _normalize_pyish(obj: Any) -> Any:\n",
    "    \"\"\"Converte recursivamente strings 'pythonizadas' em estruturas Python.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _normalize_pyish(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [_normalize_pyish(v) for v in obj]\n",
    "    if isinstance(obj, str):\n",
    "        val = _try_literal_eval(obj)\n",
    "        # se mudou de tipo, normaliza de novo (pode ter aninhado)\n",
    "        if val is not obj:\n",
    "            return _normalize_pyish(val)\n",
    "        return obj\n",
    "    return obj\n",
    "\n",
    "def read_jsonl_lines(path: str) -> List[Any]:\n",
    "    \"\"\"Lê JSONL tolerante e normaliza campos-string que são dict/list pythonizados.\"\"\"\n",
    "    out: List[Any] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # linha não é JSON puro; tenta avaliar direto como python literal\n",
    "                obj = _try_literal_eval(line)\n",
    "            obj = _normalize_pyish(obj)\n",
    "            out.append(obj)\n",
    "    return out\n",
    "\n",
    "def event_to_text(evt: Any) -> str:\n",
    "    \"\"\"Extrai texto útil do evento (dict esperado, mas aceita outros tipos).\"\"\"\n",
    "    if isinstance(evt, dict):\n",
    "        parts = []\n",
    "\n",
    "        # winlog.task\n",
    "        winlog = evt.get(\"winlog\")\n",
    "        if isinstance(winlog, dict):\n",
    "            task = winlog.get(\"task\")\n",
    "            if task: parts.append(str(task))\n",
    "        elif isinstance(winlog, str):\n",
    "            parts.append(winlog)\n",
    "\n",
    "        # process.command_line / process.name\n",
    "        proc = evt.get(\"process\")\n",
    "        if isinstance(proc, dict):\n",
    "            cmd = proc.get(\"command_line\")\n",
    "            name = proc.get(\"name\")\n",
    "            if name: parts.append(str(name))\n",
    "            if cmd:  parts.append(str(cmd))\n",
    "        elif isinstance(proc, str):\n",
    "            parts.append(proc)\n",
    "\n",
    "        # file.path/name/target_path\n",
    "        fobj = evt.get(\"file\")\n",
    "        if isinstance(fobj, dict):\n",
    "            for k in (\"path\", \"name\", \"target_path\"):\n",
    "                v = fobj.get(k)\n",
    "                if v: parts.append(str(v))\n",
    "        elif isinstance(fobj, str):\n",
    "            parts.append(fobj)\n",
    "\n",
    "        if parts:\n",
    "            return \" \".join(parts)\n",
    "        # fallback: serializa curto\n",
    "        try: return json.dumps(evt, ensure_ascii=False)\n",
    "        except Exception: return str(evt)\n",
    "\n",
    "    if isinstance(evt, list):\n",
    "        return \"\\n\".join(event_to_text(x) for x in evt)\n",
    "    if isinstance(evt, str):\n",
    "        return evt\n",
    "    try: return json.dumps(evt, ensure_ascii=False)\n",
    "    except Exception: return str(evt)\n",
    "\n",
    "def file_to_document_text(path: str) -> str:\n",
    "    events = read_jsonl_lines(path)\n",
    "    texts = [event_to_text(e) for e in events]\n",
    "    return \"\\n\".join(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288127d",
   "metadata": {},
   "source": [
    "### Montagem do dataset (e fatiamento do benigno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37f54e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1164,\n",
       " ['./attacks\\\\20250815_164702.jsonl',\n",
       "  './attacks\\\\20250815_164734.jsonl',\n",
       "  './attacks\\\\20250815_164800.jsonl'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTACK_GLOB = \"./attacks/*.jsonl\"\n",
    "BENIGN_FILE = \"./benign/benign.jsonl\"\n",
    "\n",
    "attack_files = sorted(glob.glob(ATTACK_GLOB))\n",
    "assert len(attack_files) > 0, \"Nenhum arquivo encontrado em ./attacks/*.jsonl\"\n",
    "\n",
    "assert os.path.exists(BENIGN_FILE), \"Arquivo benigno ./benign/benign.jsonl não encontrado\"\n",
    "\n",
    "len(attack_files), attack_files[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6195472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mediana de linhas (ataque): 31\n",
      "Linhas do benigno: 10000\n",
      "Numero de fatias benignas (K): 500\n"
     ]
    }
   ],
   "source": [
    "attack_line_counts = [count_lines(p) for p in attack_files]\n",
    "median_attack_lines = int(np.median(attack_line_counts)) if attack_line_counts else 500\n",
    "\n",
    "benign_total_lines = count_lines(BENIGN_FILE)\n",
    "\n",
    "# número de pedaços benignos ~ benign_total / mediana_ataque, limitado para não explodir\n",
    "K = 500\n",
    "\n",
    "print(f\"Mediana de linhas (ataque): {median_attack_lines}\")\n",
    "print(f\"Linhas do benigno: {benign_total_lines}\")\n",
    "print(f\"Numero de fatias benignas (K): {K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3701b36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documentos: 1664 | Maliciosos: 1164 | Benignos: 500\n"
     ]
    }
   ],
   "source": [
    "# 1) Documentos de ataque (label=1)\n",
    "X_text, y, ids = [], [], []\n",
    "\n",
    "for path in attack_files:\n",
    "    doc = file_to_document_text(path)\n",
    "    X_text.append(doc)\n",
    "    y.append(1)\n",
    "    ids.append(Path(path).name)\n",
    "\n",
    "# 2) Documentos benignos (label=0), fatiando o arquivo grande\n",
    "benign_lines_raw = read_jsonl_lines(BENIGN_FILE)\n",
    "chunk_size = math.ceil(len(benign_lines_raw) / K)\n",
    "\n",
    "for i in range(K):\n",
    "    chunk = benign_lines_raw[i*chunk_size : (i+1)*chunk_size]\n",
    "    if not chunk:\n",
    "        continue\n",
    "    # transformar o chunk em texto\n",
    "    chunk_texts = [event_to_text(evt) or repr(evt) for evt in chunk]\n",
    "    doc = \"\\n\".join(chunk_texts)\n",
    "    X_text.append(doc)\n",
    "    y.append(0)\n",
    "    ids.append(f\"benign_chunk_{i:03d}\")\n",
    "\n",
    "print(f\"Total documentos: {len(X_text)} | Maliciosos: {sum(y)} | Benignos: {len(y)-sum(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "47b26c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>20250818_023855.jsonl</td>\n",
       "      <td>1</td>\n",
       "      <td>6018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>20250815_195101.jsonl</td>\n",
       "      <td>1</td>\n",
       "      <td>7083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>20250815_191937.jsonl</td>\n",
       "      <td>1</td>\n",
       "      <td>3653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>benign_chunk_058</td>\n",
       "      <td>0</td>\n",
       "      <td>1749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>20250815_222414.jsonl</td>\n",
       "      <td>1</td>\n",
       "      <td>1653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>20250826_175043.jsonl</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>benign_chunk_387</td>\n",
       "      <td>0</td>\n",
       "      <td>1819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>20250815_222936.jsonl</td>\n",
       "      <td>1</td>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>20250816_011100.jsonl</td>\n",
       "      <td>1</td>\n",
       "      <td>5972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>benign_chunk_234</td>\n",
       "      <td>0</td>\n",
       "      <td>2041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  label  n_chars\n",
       "845   20250818_023855.jsonl      1     6018\n",
       "205   20250815_195101.jsonl      1     7083\n",
       "137   20250815_191937.jsonl      1     3653\n",
       "1222       benign_chunk_058      0     1749\n",
       "492   20250815_222414.jsonl      1     1653\n",
       "960   20250826_175043.jsonl      1        0\n",
       "1551       benign_chunk_387      0     1819\n",
       "508   20250815_222936.jsonl      1      804\n",
       "789   20250816_011100.jsonl      1     5972\n",
       "1398       benign_chunk_234      0     2041"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"id\": ids, \"label\": y, \"n_chars\": [len(t) for t in X_text]})\n",
    "df.sample(min(10, len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa029141",
   "metadata": {},
   "source": [
    "### Split e avaliação auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56109390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_test, pred, average=\"binary\", zero_division=0)\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\"Accuracy: {acc:.3f} | Precision: {pr:.3f} | Recall: {rc:.3f} | F1: {f1:.3f}\")\n",
    "    print(classification_report(y_test, pred, digits=3))\n",
    "    return {\"model\": name, \"accuracy\": acc, \"precision\": pr, \"recall\": rc, \"f1\": f1}\n",
    "\n",
    "# Split estratificado\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Vetorizador compartilhado\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_features=100_000,   # limitar um pouco\n",
    "    ngram_range=(1,2),      # unigrams e bigrams\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dec537",
   "metadata": {},
   "source": [
    "### Treinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "30bf82cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LogisticRegression]\n",
      "Accuracy: 0.991 | Precision: 0.987 | Recall: 1.000 | F1: 0.994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.970     0.985       100\n",
      "           1      0.987     1.000     0.994       233\n",
      "\n",
      "    accuracy                          0.991       333\n",
      "   macro avg      0.994     0.985     0.989       333\n",
      "weighted avg      0.991     0.991     0.991       333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (regressor \"softmax\" no multi-classe; aqui binário)\n",
    "logreg = make_pipeline(\n",
    "    vectorizer,\n",
    "    LogisticRegression(max_iter=500, n_jobs=None)  # simples\n",
    ")\n",
    "m1 = evaluate_model(\"LogisticRegression\", logreg, X_train_text, X_test_text, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e6fc7bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LinearSVC]\n",
      "Accuracy: 0.997 | Precision: 0.996 | Recall: 1.000 | F1: 0.998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.990     0.995       100\n",
      "           1      0.996     1.000     0.998       233\n",
      "\n",
      "    accuracy                          0.997       333\n",
      "   macro avg      0.998     0.995     0.996       333\n",
      "weighted avg      0.997     0.997     0.997       333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear SVM\n",
    "linsvm = make_pipeline(\n",
    "    vectorizer,\n",
    "    LinearSVC()\n",
    ")\n",
    "m2 = evaluate_model(\"LinearSVC\", linsvm, X_train_text, X_test_text, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7716503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DecisionTree]\n",
      "Accuracy: 1.000 | Precision: 1.000 | Recall: 1.000 | F1: 1.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000       100\n",
      "           1      1.000     1.000     1.000       233\n",
      "\n",
      "    accuracy                          1.000       333\n",
      "   macro avg      1.000     1.000     1.000       333\n",
      "weighted avg      1.000     1.000     1.000       333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree (tende a underfitting com TF-IDF esparso, mas incluímos)\n",
    "dtree = make_pipeline(\n",
    "    vectorizer,\n",
    "    DecisionTreeClassifier(random_state=42, max_depth=30)\n",
    ")\n",
    "m3 = evaluate_model(\"DecisionTree\", dtree, X_train_text, X_test_text, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "711d0dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RandomForest]\n",
      "Accuracy: 1.000 | Precision: 1.000 | Recall: 1.000 | F1: 1.000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000       100\n",
      "           1      1.000     1.000     1.000       233\n",
      "\n",
      "    accuracy                          1.000       333\n",
      "   macro avg      1.000     1.000     1.000       333\n",
      "weighted avg      1.000     1.000     1.000       333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Random Forest (com TF-IDF esparso nem sempre é ideal, mas funciona)\n",
    "rf = make_pipeline(\n",
    "    vectorizer,\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_depth=None\n",
    "    )\n",
    ")\n",
    "m4 = evaluate_model(\"RandomForest\", rf, X_train_text, X_test_text, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73353b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.996997</td>\n",
       "      <td>0.995726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.987288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  accuracy  precision  recall        f1\n",
       "3        RandomForest  1.000000   1.000000     1.0  1.000000\n",
       "2        DecisionTree  1.000000   1.000000     1.0  1.000000\n",
       "1           LinearSVC  0.996997   0.995726     1.0  0.997859\n",
       "0  LogisticRegression  0.990991   0.987288     1.0  0.993603"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tabela comparativa de métricas\n",
    "score_table = pd.DataFrame([m1, m2, m3, m4]).sort_values(\"f1\", ascending=False)\n",
    "score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d978717",
   "metadata": {},
   "source": [
    "### Salvar o melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3baa125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo: ./models/model_LogisticRegression.joblib\n",
      "Salvo: ./models/model_LinearSVC.joblib\n",
      "Salvo: ./models/model_DecisionTree.joblib\n",
      "Salvo: ./models/model_RandomForest.joblib\n",
      "Salvo melhor modelo em: ./models/best_model_RandomForest.joblib\n",
      "Salvo resumo de métricas em: ./models/metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Salvar todos os modelos e também destacar o melhor\n",
    "import os, joblib\n",
    "\n",
    "# dicionário com todos os pipelines treinados\n",
    "all_models = {\n",
    "    \"LogisticRegression\": logreg,\n",
    "    \"LinearSVC\": linsvm,\n",
    "    \"DecisionTree\": dtree,\n",
    "    \"RandomForest\": rf,\n",
    "}\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "# salva cada um\n",
    "for name, pipe in all_models.items():\n",
    "    path = f\"./models/model_{name}.joblib\"\n",
    "    joblib.dump(pipe, path)\n",
    "    print(\"Salvo:\", path)\n",
    "\n",
    "# opcional: salva também o melhor (com base no score_table)\n",
    "best_name = score_table.iloc[0][\"model\"]\n",
    "best_pipe = all_models[best_name]\n",
    "best_path = f\"./models/best_model_{best_name}.joblib\"\n",
    "joblib.dump(best_pipe, best_path)\n",
    "print(\"Salvo melhor modelo em:\", best_path)\n",
    "\n",
    "# opcional: salvar a tabela de métricas para referência\n",
    "score_table.to_csv(\"./models/metrics_summary.csv\", index=False)\n",
    "print(\"Salvo resumo de métricas em: ./models/metrics_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bad00",
   "metadata": {},
   "source": [
    "### Inferência em novos arquivos/diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe2ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
    "\n",
    "\n",
    "def predict_file_label(model, path: str) -> int:\n",
    "    \"\"\"\n",
    "    Lê um arquivo JSONL, transforma em texto (via file_to_document_text),\n",
    "    e usa o modelo para prever se o arquivo é malicioso (1) ou benigno (0).\n",
    "    \"\"\"\n",
    "    doc = file_to_document_text(path)\n",
    "    pred = model.predict([doc])[0]\n",
    "    return int(pred)\n",
    "\n",
    "def batch_predict_dir(model, pattern: str = \"../data/new_samples/*.jsonl\", true_label: int|None=None, save_csv: str|None=None):\n",
    "    \"\"\"\n",
    "    Roda predição em lote em todos os arquivos que batem com o padrão `pattern`.\n",
    "    \n",
    "    Args:\n",
    "        model: modelo sklearn já treinado/carregado.\n",
    "        pattern: caminho com wildcard para os arquivos (ex: \"../data/attack/*.jsonl\").\n",
    "        true_label: se informado (0 ou 1), calcula métricas de avaliação.\n",
    "        save_csv: caminho opcional para salvar os resultados em CSV.\n",
    "\n",
    "    Returns:\n",
    "        df_pred: DataFrame com resultados (file, pred, true, status).\n",
    "    \"\"\"\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        print(\"Nenhum arquivo encontrado para inferência.\")\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "    for p in files:\n",
    "        try:\n",
    "            yhat = predict_file_label(model, p)\n",
    "            rows.append({\n",
    "                \"file\": Path(p).name,\n",
    "                \"pred\": yhat,\n",
    "                \"true\": true_label,\n",
    "                \"status\": \"ok\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            rows.append({\n",
    "                \"file\": Path(p).name,\n",
    "                \"pred\": None,\n",
    "                \"true\": true_label,\n",
    "                \"status\": f\"erro: {type(e).__name__}: {e}\"\n",
    "            })\n",
    "            print(f\"{Path(p).name:40s} => ERRO ({type(e).__name__}: {e})\")\n",
    "\n",
    "    df_pred = pd.DataFrame(rows)\n",
    "\n",
    "    # Métricas apenas nos que deram certo\n",
    "    if true_label is not None:\n",
    "        ok = df_pred[df_pred[\"status\"] == \"ok\"]\n",
    "        if len(ok) > 0:\n",
    "            acc = accuracy_score(ok[\"true\"], ok[\"pred\"])\n",
    "            pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "                ok[\"true\"], ok[\"pred\"], average=\"binary\", zero_division=0\n",
    "            )\n",
    "            print(f\"\\nEval (somente arquivos OK; rótulo verdadeiro={true_label}): \"\n",
    "                  f\"Acc={acc:.3f} | P={pr:.3f} | R={rc:.3f} | F1={f1:.3f}\")\n",
    "        else:\n",
    "            print(\"\\nNão houve arquivos válidos para calcular métricas.\")\n",
    "\n",
    "    # salvar resultados em CSV se pedido\n",
    "    if save_csv:\n",
    "        df_pred.to_csv(save_csv, index=False)\n",
    "        print(f\"Resultados salvos em {save_csv}\")\n",
    "\n",
    "    return df_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Avaliando best_model_RandomForest =====\n",
      "\n",
      "-- Conjunto: attack | pattern=../../src/data/attack/*.jsonl | true=1\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=1): Acc=0.951 | P=1.000 | R=0.951 | F1=0.975\n",
      "\n",
      "-- Conjunto: safe | pattern=../../src/data/safe/*.jsonl | true=0\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=0): Acc=0.000 | P=0.000 | R=0.000 | F1=0.000\n",
      "\n",
      "===== Avaliando model_DecisionTree =====\n",
      "\n",
      "-- Conjunto: attack | pattern=../../src/data/attack/*.jsonl | true=1\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=1): Acc=0.951 | P=1.000 | R=0.951 | F1=0.975\n",
      "\n",
      "-- Conjunto: safe | pattern=../../src/data/safe/*.jsonl | true=0\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=0): Acc=0.211 | P=0.000 | R=0.000 | F1=0.000\n",
      "\n",
      "===== Avaliando model_LinearSVC =====\n",
      "\n",
      "-- Conjunto: attack | pattern=../../src/data/attack/*.jsonl | true=1\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=1): Acc=0.951 | P=1.000 | R=0.951 | F1=0.975\n",
      "\n",
      "-- Conjunto: safe | pattern=../../src/data/safe/*.jsonl | true=0\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=0): Acc=0.053 | P=0.000 | R=0.000 | F1=0.000\n",
      "\n",
      "===== Avaliando model_LogisticRegression =====\n",
      "\n",
      "-- Conjunto: attack | pattern=../../src/data/attack/*.jsonl | true=1\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=1): Acc=0.951 | P=1.000 | R=0.951 | F1=0.975\n",
      "\n",
      "-- Conjunto: safe | pattern=../../src/data/safe/*.jsonl | true=0\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=0): Acc=0.105 | P=0.000 | R=0.000 | F1=0.000\n",
      "\n",
      "===== Avaliando model_RandomForest =====\n",
      "\n",
      "-- Conjunto: attack | pattern=../../src/data/attack/*.jsonl | true=1\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=1): Acc=0.951 | P=1.000 | R=0.951 | F1=0.975\n",
      "\n",
      "-- Conjunto: safe | pattern=../../src/data/safe/*.jsonl | true=0\n",
      "\n",
      "Eval (somente arquivos OK; rótulo verdadeiro=0): Acc=0.000 | P=0.000 | R=0.000 | F1=0.000\n",
      "\n",
      "=== RESUMO DE MÉTRICAS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>set</th>\n",
       "      <th>n_ok</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model_DecisionTree</td>\n",
       "      <td>ALL</td>\n",
       "      <td>60</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.821053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model_LogisticRegression</td>\n",
       "      <td>ALL</td>\n",
       "      <td>60</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.804124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model_LinearSVC</td>\n",
       "      <td>ALL</td>\n",
       "      <td>60</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.795918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best_model_RandomForest</td>\n",
       "      <td>ALL</td>\n",
       "      <td>60</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model_RandomForest</td>\n",
       "      <td>ALL</td>\n",
       "      <td>60</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best_model_RandomForest</td>\n",
       "      <td>attack</td>\n",
       "      <td>41</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model_DecisionTree</td>\n",
       "      <td>attack</td>\n",
       "      <td>41</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model_LinearSVC</td>\n",
       "      <td>attack</td>\n",
       "      <td>41</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model_LogisticRegression</td>\n",
       "      <td>attack</td>\n",
       "      <td>41</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model_RandomForest</td>\n",
       "      <td>attack</td>\n",
       "      <td>41</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.95122</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best_model_RandomForest</td>\n",
       "      <td>safe</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model_DecisionTree</td>\n",
       "      <td>safe</td>\n",
       "      <td>19</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model_LinearSVC</td>\n",
       "      <td>safe</td>\n",
       "      <td>19</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model_LogisticRegression</td>\n",
       "      <td>safe</td>\n",
       "      <td>19</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model_RandomForest</td>\n",
       "      <td>safe</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model     set  n_ok  accuracy  precision   recall  \\\n",
       "5         model_DecisionTree     ALL    60  0.716667   0.722222  0.95122   \n",
       "11  model_LogisticRegression     ALL    60  0.683333   0.696429  0.95122   \n",
       "8            model_LinearSVC     ALL    60  0.666667   0.684211  0.95122   \n",
       "2    best_model_RandomForest     ALL    60  0.650000   0.672414  0.95122   \n",
       "14        model_RandomForest     ALL    60  0.650000   0.672414  0.95122   \n",
       "0    best_model_RandomForest  attack    41  0.951220   1.000000  0.95122   \n",
       "3         model_DecisionTree  attack    41  0.951220   1.000000  0.95122   \n",
       "6            model_LinearSVC  attack    41  0.951220   1.000000  0.95122   \n",
       "9   model_LogisticRegression  attack    41  0.951220   1.000000  0.95122   \n",
       "12        model_RandomForest  attack    41  0.951220   1.000000  0.95122   \n",
       "1    best_model_RandomForest    safe    19  0.000000   0.000000  0.00000   \n",
       "4         model_DecisionTree    safe    19  0.210526   0.000000  0.00000   \n",
       "7            model_LinearSVC    safe    19  0.052632   0.000000  0.00000   \n",
       "10  model_LogisticRegression    safe    19  0.105263   0.000000  0.00000   \n",
       "13        model_RandomForest    safe    19  0.000000   0.000000  0.00000   \n",
       "\n",
       "          f1  \n",
       "5   0.821053  \n",
       "11  0.804124  \n",
       "8   0.795918  \n",
       "2   0.787879  \n",
       "14  0.787879  \n",
       "0   0.975000  \n",
       "3   0.975000  \n",
       "6   0.975000  \n",
       "9   0.975000  \n",
       "12  0.975000  \n",
       "1   0.000000  \n",
       "4   0.000000  \n",
       "7   0.000000  \n",
       "10  0.000000  \n",
       "13  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório salvo em: ./models/eval_reports/all_models_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Avaliar TODOS os modelos salvos contra TODOS os conjuntos em ../filtering/<filtro>/{attack,safe}\n",
    "import os, glob, joblib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ----------------------------------------\n",
    "# Descobre automaticamente os conjuntos:\n",
    "# ../filtering/<filter_model>/attack/*.jsonl\n",
    "# ../filtering/<filter_model>/safe/*.jsonl\n",
    "# ----------------------------------------\n",
    "def discover_filtering_eval_sets(base_dir: str = \"../filtering\"):\n",
    "    \"\"\"\n",
    "    Retorna uma lista de dicts:\n",
    "    [{'pattern': '.../*.jsonl', 'true': 1/0, 'set': 'attack'/'safe', 'filter_model': '<nome_pasta>'}, ...]\n",
    "    \"\"\"\n",
    "    eval_sets = []\n",
    "    base_path = Path(base_dir)\n",
    "    if not base_path.exists():\n",
    "        print(f\"Aviso: {base_dir} não encontrado.\")\n",
    "        return eval_sets\n",
    "\n",
    "    for sub in sorted(p for p in base_path.iterdir() if p.is_dir()):\n",
    "        filter_model = sub.name  # ex.: gemma3_4b\n",
    "        attack_dir = sub / \"attack\"\n",
    "        safe_dir   = sub / \"safe\"\n",
    "\n",
    "        if attack_dir.exists():\n",
    "            eval_sets.append({\n",
    "                \"pattern\": str(attack_dir / \"*.jsonl\"),\n",
    "                \"true\": 1,\n",
    "                \"set\": \"attack\",\n",
    "                \"filter_model\": filter_model\n",
    "            })\n",
    "        if safe_dir.exists():\n",
    "            eval_sets.append({\n",
    "                \"pattern\": str(safe_dir / \"*.jsonl\"),\n",
    "                \"true\": 0,\n",
    "                \"set\": \"safe\",\n",
    "                \"filter_model\": filter_model\n",
    "            })\n",
    "    return eval_sets\n",
    "\n",
    "\n",
    "def eval_df_metrics(df: pd.DataFrame):\n",
    "    \"\"\"Computa métricas binárias em um DataFrame de previsões (somente linhas OK).\"\"\"\n",
    "    ok = df[df[\"status\"] == \"ok\"].copy()\n",
    "    if \"true\" not in ok or ok[\"true\"].isna().all() or len(ok) == 0:\n",
    "        return {\"n_ok\": len(ok), \"accuracy\": None, \"precision\": None, \"recall\": None, \"f1\": None}\n",
    "    acc = accuracy_score(ok[\"true\"], ok[\"pred\"])\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(ok[\"true\"], ok[\"pred\"], average=\"binary\", zero_division=0)\n",
    "    return {\"n_ok\": len(ok), \"accuracy\": acc, \"precision\": pr, \"recall\": rc, \"f1\": f1}\n",
    "\n",
    "\n",
    "def evaluate_all_models(models_dir: str = \"./models\", filtering_base: str = \"../filtering\"):\n",
    "    \"\"\"\n",
    "    Carrega todos os modelos sklearn salvos em ./models/*.joblib\n",
    "    e avalia em todos os conjuntos descobertos dentro de ../filtering/*/{attack,safe}.\n",
    "    Retorna (metrics_df, preds_concat).\n",
    "    \"\"\"\n",
    "    model_paths = sorted(glob.glob(os.path.join(models_dir, \"*.joblib\")))\n",
    "    assert model_paths, f\"Nenhum modelo encontrado em {models_dir}\"\n",
    "\n",
    "    # Descobrir conjuntos\n",
    "    eval_sets = discover_filtering_eval_sets(filtering_base)\n",
    "    assert eval_sets, f\"Nenhum conjunto encontrado em {filtering_base}\"\n",
    "\n",
    "    per_model_rows = []   # métricas por (sk_model) x (filter_model) x (set)\n",
    "    overall_rows   = []   # concat de previsões, para análises adicionais\n",
    "\n",
    "    for mp in model_paths:\n",
    "        sk_model_name = Path(mp).stem  # ex.: model_LinearSVC, best_model_LogisticRegression\n",
    "        print(f\"\\n===== Avaliando {sk_model_name} =====\")\n",
    "        clf = joblib.load(mp)\n",
    "\n",
    "        # Guardar previsões por filtro para depois calcular 'ALL' por filtro e por modelo\n",
    "        preds_per_filter = {}\n",
    "\n",
    "        for es in eval_sets:\n",
    "            pattern      = es[\"pattern\"]\n",
    "            true_label   = es[\"true\"]\n",
    "            set_name     = es[\"set\"]\n",
    "            filter_model = es[\"filter_model\"]\n",
    "\n",
    "            print(f\"\\n-- Filtro: {filter_model} | Conjunto: {set_name} | true={true_label}\")\n",
    "            df_pred = batch_predict_dir(clf, pattern, true_label=true_label)\n",
    "            if df_pred is None:\n",
    "                continue\n",
    "\n",
    "            df_pred = df_pred.copy()\n",
    "            df_pred[\"set\"]          = set_name\n",
    "            df_pred[\"filter_model\"] = filter_model\n",
    "            df_pred[\"sk_model\"]     = sk_model_name\n",
    "\n",
    "            # Acumula previsões por filtro\n",
    "            preds_per_filter.setdefault(filter_model, []).append(df_pred)\n",
    "            overall_rows.append(df_pred)\n",
    "\n",
    "            # Métricas por (sk_model, filter_model, set)\n",
    "            m = eval_df_metrics(df_pred)\n",
    "            per_model_rows.append({\n",
    "                \"sk_model\": sk_model_name,\n",
    "                \"filter_model\": filter_model,\n",
    "                \"set\": set_name,\n",
    "                **m\n",
    "            })\n",
    "\n",
    "        # Métricas 'ALL' por filtro (juntando attack+safe) para este sk_model\n",
    "        for fmodel, dfs in preds_per_filter.items():\n",
    "            cat = pd.concat(dfs, ignore_index=True)\n",
    "            cat_known = cat[(cat[\"status\"] == \"ok\") & (~cat[\"true\"].isna())]\n",
    "            if len(cat_known) > 0:\n",
    "                acc = accuracy_score(cat_known[\"true\"], cat_known[\"pred\"])\n",
    "                pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "                    cat_known[\"true\"], cat_known[\"pred\"], average=\"binary\", zero_division=0\n",
    "                )\n",
    "                per_model_rows.append({\n",
    "                    \"sk_model\": sk_model_name,\n",
    "                    \"filter_model\": fmodel,\n",
    "                    \"set\": \"ALL\",\n",
    "                    \"n_ok\": len(cat_known),\n",
    "                    \"accuracy\": acc,\n",
    "                    \"precision\": pr,\n",
    "                    \"recall\": rc,\n",
    "                    \"f1\": f1\n",
    "                })\n",
    "\n",
    "    # Tabela de métricas final\n",
    "    metrics_df = pd.DataFrame(per_model_rows)\n",
    "    if not metrics_df.empty:\n",
    "        metrics_df = metrics_df.sort_values([\"filter_model\", \"set\", \"f1\"], ascending=[True, True, False])\n",
    "\n",
    "    print(\"\\n=== RESUMO DE MÉTRICAS (por filtro / conjunto / modelo) ===\")\n",
    "    try:\n",
    "        display(metrics_df)\n",
    "    except:\n",
    "        print(metrics_df.head())\n",
    "\n",
    "    # Salvar relatórios\n",
    "    out_dir = \"./models/eval_reports\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # 1) CSV único com tudo\n",
    "    out_csv_all = os.path.join(out_dir, \"all_models_metrics_by_filter.csv\")\n",
    "    metrics_df.to_csv(out_csv_all, index=False)\n",
    "    print(\"Relatório geral salvo em:\", out_csv_all)\n",
    "\n",
    "    # 2) Um CSV por filtro (opcional, útil para inspeção separada)\n",
    "    for fmodel in sorted(metrics_df[\"filter_model\"].dropna().unique()):\n",
    "        sub = metrics_df[metrics_df[\"filter_model\"] == fmodel]\n",
    "        out_csv = os.path.join(out_dir, f\"metrics_{fmodel}.csv\")\n",
    "        sub.to_csv(out_csv, index=False)\n",
    "        print(f\"Relatório por filtro salvo em: {out_csv}\")\n",
    "\n",
    "    preds_concat = pd.concat(overall_rows, ignore_index=True) if overall_rows else None\n",
    "    # opcional: salvar predições concatenadas\n",
    "    if preds_concat is not None and not preds_concat.empty:\n",
    "        preds_csv = os.path.join(out_dir, \"all_predictions_by_filter.csv\")\n",
    "        preds_concat.to_csv(preds_csv, index=False)\n",
    "        print(\"Predições concatenadas salvas em:\", preds_csv)\n",
    "\n",
    "    return metrics_df, preds_concat\n",
    "\n",
    "# Executar\n",
    "all_metrics, all_preds = evaluate_all_models(\n",
    "    models_dir=\"./models\",\n",
    "    filtering_base=\"../filtering\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
